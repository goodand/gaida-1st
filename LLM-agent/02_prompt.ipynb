{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d5f9b76",
   "metadata": {},
   "source": [
    "# 프롬프트(Prompts)\n",
    "    02_prompt.ipynb\n",
    "\n",
    "- LLM한테 주는 입력(지시, 맥락, 지억)\n",
    "- 지시 :'~해줘'\n",
    "- 맥락 : context - 현재 지시를 위해 제공하는 추가 정보\n",
    "- 기억 : Memory - 지금까지 했던 대화 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ab22ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628cccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.13/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain_openai in ./.venv/lib/python3.13/site-packages (0.3.32)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in ./.venv/lib/python3.13/site-packages (from langchain) (0.3.75)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./.venv/lib/python3.13/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in ./.venv/lib/python3.13/site-packages (from langchain) (0.4.21)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.13/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.13/site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.13/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in ./.venv/lib/python3.13/site-packages (from langchain_openai) (1.102.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.venv/lib/python3.13/site-packages (from langchain_openai) (0.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (0.16.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2025.8.29)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda9970",
   "metadata": {},
   "source": [
    "## PromptTemplate\n",
    "- 단순히 1회성 명령을 내릴 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9829221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ab96df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도가 어디인가요?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추후에 체인.invoke에서 {}내부에 들어갈 말을 채워줘야 함\n",
    "\n",
    "template = '{country}의 수도가 어디인가요?'\n",
    "\n",
    "# .from_template 메서드로 프롬프트 만들기\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# {}를 채우는 방법 (우리는 결국 chain.invoke로 쓰게 됨)\n",
    "\n",
    "prompt.format(country='대한민국')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "868b573c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 서울입니다.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({'country':'대한민국'}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c1dc4",
   "metadata": {},
   "source": [
    "# Partial Variable (부분 변수)\n",
    "- 프롬프트에서 매개변수 기본값 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927f255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국과 미국의 수도는 각각 어디인가요?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = '{c1}과 {c2}의 수도는 각각 어디인가요?'\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['c1'],\n",
    "    partial_variables={\n",
    "        'c2': '미국'\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt.format(c1='한국')\n",
    "# prompt.format(c2='캐나다') \n",
    "\n",
    "# PromptTemplate() 클래스의 역할은 ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffb02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: 'hello'\n",
      "출력: 'hello'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# invoke 메쏘드 실행 \n",
    "\n",
    "# RunnablePassthrough 인스턴스 생성\n",
    "passthrough_runnable = RunnablePassthrough()\n",
    "\n",
    "# invoke() 메서드를 사용하여 'hello'를 입력으로 전달\n",
    "output = passthrough_runnable.invoke(\"hello\")\n",
    "\n",
    "print(f\"입력: 'hello'\")\n",
    "print(f\"출력: '{output}'\")\n",
    "\n",
    "# 결과:\n",
    "# 입력: 'hello'\n",
    "# 출력: 'hello'\n",
    "\n",
    "\n",
    "# () 괄호는 항상 \"무언가를 실행하라\"는 의미를 내포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620fd80",
   "metadata": {},
   "source": [
    "Runnable의 역할\n",
    "RunnableLambda 클래스는 Runnable이라는 규격(인터페이스)을 따릅니다. 이 규격은 invoke(), stream(), batch()와 같은 메서드들을 가지고 있어야 한다고 정의합니다.\n",
    "\n",
    "당신이 add_one 함수를 RunnableLambda에 넣어서 lambda_runnable이라는 객체를 만들면, 이 객체는 Runnable의 모든 속성과 메서드를 상속받아 자동으로 invoke(), stream(), batch()를 사용할 수 있는 능력을 갖게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72554cf4",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "채팅을 주고받는 템플릿 생성용\n",
    "대화 목록을 LLM에게 주입\n",
    "하나의 Chat 은 role 과 message 로 구성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc6e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 한국의 수도는 어디?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template('{country}의 수도는 어디?')\n",
    "chat_prompt.format(country='한국')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddd4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===format===\n",
      "System: 당신은 친절한 AI어시스트. 이름은 gaida 야.\n",
      "Human: 반가워!\n",
      "AI: 무엇을 도와드릴까요?\n",
      "Human: 이름이 뭐니?\n",
      "===format_message===\n",
      "[SystemMessage(content='당신은 친절한 AI어시스트. 이름은 gaida 야.', additional_kwargs={}, response_metadata={}), HumanMessage(content='반가워!', additional_kwargs={}, response_metadata={}), AIMessage(content='무엇을 도와드릴까요?', additional_kwargs={}, response_metadata={}), HumanMessage(content='이름이 뭐니?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # role - message\n",
    "        ('system', '당신은 친절한 AI어시스트. 이름은 {name} 야.'),\n",
    "        ('human', '반가워!'),\n",
    "        ('ai', '무엇을 도와드릴까요?'),\n",
    "        ('human', '{user_input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# format vs format_messages\n",
    "messages_str = chat_template.format(name='gaida', user_input='이름이 뭐니?')\n",
    "messages_cls = chat_template.format_messages(name='gaida', user_input='이름이 뭐니?')\n",
    "print('===format===') #  스트팅\n",
    "print(messages_str)\n",
    "print('===format_message===') # 리스트 타입으로 반환\n",
    "print(messages_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f8a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 저는 gaida라고 해요. 반가워요!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한덩어리 텍스트\n",
    "llm.invoke(messages_str).content # ''안녕하세요! 저는 gaida라고 해요. 반가워요!''\n",
    "# 실제 대화 내역으로 인지 (Langsmith 가서 확인)\n",
    "llm.invoke(messages_cls).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4a7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'반가워요, 탁재현님! 이름점은 재밌는 점검이지만, 저는 전문적인 점술가가 아니어서 정확한 점을 볼 수는 없어요. 대신, 재현이라는 이름이 갖는 의미나 느낌에 대해 이야기해 드릴 수 있어요. 탁재현님은 특별하고 강한 인상을 주는 이름 같아요! 혹시 이름에 대해 더 알고 싶거나 다른 궁금한 점이 있나요?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_template | llm | StrOutputParser() \n",
    "\n",
    "chain.invoke({'name': '가이다', 'user_input': '내 이름은 탁재현. 너와 나의 이름점을 봐줘'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91198f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### 1\\. content: 텍스트 응답\n",
    "\n",
    "이것이 LLM 응답의 가장 기본적인 부분입니다.\n",
    "\n",
    "**예시:**\n",
    "\n",
    "```json\n",
    "\"content\": \"제가 가진 정보에 따르면, 대한민국은 동아시아에 위치한 나라입니다.\"\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### 2\\. additional\\_kwargs: 추가적인 정보\n",
    "\n",
    "주로 모델이 함수(도구, tool)를 사용하도록 지시하거나, 특별한 응답을 할 때 나타납니다.\n",
    "\n",
    "**예시 (도구 호출):**\n",
    "\n",
    "```json\n",
    "\"additional_kwargs\": {\n",
    "  \"tool_calls\": [\n",
    "    {\n",
    "      \"id\": \"call_abc123\",\n",
    "      \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"arguments\": \"{\\\"location\\\": \\\"서울\\\"}\"\n",
    "      },\n",
    "      \"type\": \"function\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "이 예시는 모델이 \"get\\_current\\_weather\"라는 이름의 함수를 \"서울\"을 인수로 사용하여 호출하라고 지시하는 상황을 보여줍니다.\n",
    "\n",
    "-----\n",
    "\n",
    "### 3\\. response\\_metadata: API 호출 정보\n",
    "\n",
    "응답 내용과는 별개로, API 사용량 및 성능과 관련된 정보입니다.\n",
    "\n",
    "**예시:**\n",
    "\n",
    "```json\n",
    "\"response_metadata\": {\n",
    "  \"model_name\": \"gpt-4\",\n",
    "  \"token_usage\": {\n",
    "    \"completion_tokens\": 20,\n",
    "    \"prompt_tokens\": 100,\n",
    "    \"total_tokens\": 120\n",
    "  },\n",
    "  \"finish_reason\": \"stop\",\n",
    "  \"logprobs\": null\n",
    "}\n",
    "```\n",
    "\n",
    "이 예시는 사용된 모델의 이름, 프롬프트와 응답에 사용된 토큰 수, 그리고 응답이 끝난 이유 등을 담고 있습니다.\n",
    "\n",
    "-----\n",
    "\n",
    "### 4\\. tool\\_calls: 모델이 호출한 도구 정보\n",
    "\n",
    "`additional_kwargs` 안에 포함되거나, 별도의 속성으로 나올 수 있습니다. `additional_kwargs` 예시와 동일한 정보를 보여줍니다.\n",
    "\n",
    "**예시:**\n",
    "\n",
    "```json\n",
    "\"tool_calls\": [\n",
    "  {\n",
    "    \"id\": \"call_abc123\",\n",
    "    \"function\": {\n",
    "      \"name\": \"get_current_weather\",\n",
    "      \"arguments\": \"{\\\"location\\\": \\\"서울\\\"}\"\n",
    "    },\n",
    "    \"type\": \"function\"\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581465d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='반가워요, 탁재현님! 이름점은 재미있는 점술이지만, 저는 과학적 정보와 조언을 드리는 AI라서 이름점은 정확하지 않을 수 있어요. 그래도 함께 한번 살펴볼게요!\\n\\n• **탁재현**이라는 이름에서,\\n- \"탁\"은 높고 굳건한 이미지를 떠올리게 해요.\\n- \"재\"는 재물이나 재능을 의미할 수 있고,\\n- \"현\"은 빛이나 밝음을 상징하죠.\\n\\n전체적으로 보면, 재능과 밝음이 어우러진 멋진 이름이에요! 좋은 일 가득하시길 바라요. 다른 궁금한 점 있나요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 154, 'prompt_tokens': 66, 'total_tokens': 220, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_e91a518ddb', 'id': 'chatcmpl-CAwXGq6RHzhtxEP5a7r71kC9iSybf', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--11408ec5-aa37-42e1-b114-44864fb8a52d-0', usage_metadata={'input_tokens': 66, 'output_tokens': 154, 'total_tokens': 220, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_template | llm \n",
    "\n",
    "chain.invoke({'name': '가이다', 'user_input': '내 이름은 탁재현. 너와 나의 이름점을 봐줘'})\n",
    "\n",
    "# str 파서 없이도 위 코드가 실행됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cce893",
   "metadata": {},
   "source": [
    "# 프롬프팅 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf7942",
   "metadata": {},
   "source": [
    "    # 'langchain-hub'\n",
    "\n",
    "[랭스미스허브](https://smith.langchain.com/hub)\n",
    "- 다양한 사용자들이 업로드한 프롬프트를 받아서 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53c7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'} template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'\n",
      "=== d ===\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull('hwchase17/react')\n",
    "\n",
    "print(prompt)\n",
    "print('=== d ===')\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b33285",
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithUserError",
     "evalue": "Cannot create a prompt for another tenant.\nCurrent tenant: None,\nRequested tenant: 707484c1-574f-4044-921f-46b47251df9e",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLangSmithUserError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m prompt = PromptTemplate.from_template(\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m이 프롬프트의 철학은 프레게로 부터 시작되었다.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mhub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m707484c1-574f-4044-921f-46b47251df9e/llm_philosophy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gaida-1st/LLM-agent/.venv/lib/python3.13/site-packages/langchain/hub.py:72\u001b[39m, in \u001b[36mpush\u001b[39m\u001b[34m(repo_full_name, object, api_url, api_key, parent_commit_hash, new_repo_is_public, new_repo_description, readme, tags)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Then it's langsmith\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(client, \u001b[33m\"\u001b[39m\u001b[33mpush_prompt\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_full_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparent_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparent_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_public\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_repo_is_public\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_repo_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreadme\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreadme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Then it's langchainhub\u001b[39;00m\n\u001b[32m     83\u001b[39m manifest_json = dumps(\u001b[38;5;28mobject\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gaida-1st/LLM-agent/.venv/lib/python3.13/site-packages/langsmith/client.py:7692\u001b[39m, in \u001b[36mClient.push_prompt\u001b[39m\u001b[34m(self, prompt_identifier, object, parent_commit_hash, is_public, description, readme, tags)\u001b[39m\n\u001b[32m   7684\u001b[39m         \u001b[38;5;28mself\u001b[39m.update_prompt(\n\u001b[32m   7685\u001b[39m             prompt_identifier,\n\u001b[32m   7686\u001b[39m             description=description,\n\u001b[32m   (...)\u001b[39m\u001b[32m   7689\u001b[39m             is_public=is_public,\n\u001b[32m   7690\u001b[39m         )\n\u001b[32m   7691\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m7692\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_public\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_public\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_public\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   7695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreadme\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreadme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7698\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7700\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   7701\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_prompt_url(prompt_identifier=prompt_identifier)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gaida-1st/LLM-agent/.venv/lib/python3.13/site-packages/langsmith/client.py:7312\u001b[39m, in \u001b[36mClient.create_prompt\u001b[39m\u001b[34m(self, prompt_identifier, description, readme, tags, is_public)\u001b[39m\n\u001b[32m   7310\u001b[39m owner, prompt_name, _ = ls_utils.parse_prompt_identifier(prompt_identifier)\n\u001b[32m   7311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._current_tenant_is_owner(owner=owner):\n\u001b[32m-> \u001b[39m\u001b[32m7312\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._owner_conflict_error(\u001b[33m\"\u001b[39m\u001b[33mcreate a prompt\u001b[39m\u001b[33m\"\u001b[39m, owner)\n\u001b[32m   7314\u001b[39m json: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Sequence[\u001b[38;5;28mstr\u001b[39m]]] = {\n\u001b[32m   7315\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrepo_handle\u001b[39m\u001b[33m\"\u001b[39m: prompt_name,\n\u001b[32m   7316\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m: description \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   7319\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mis_public\u001b[39m\u001b[33m\"\u001b[39m: is_public,\n\u001b[32m   7320\u001b[39m }\n\u001b[32m   7322\u001b[39m response = \u001b[38;5;28mself\u001b[39m.request_with_retries(\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m/repos/\u001b[39m\u001b[33m\"\u001b[39m, json=json)\n",
      "\u001b[31mLangSmithUserError\u001b[39m: Cannot create a prompt for another tenant.\nCurrent tenant: None,\nRequested tenant: 707484c1-574f-4044-921f-46b47251df9e"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    '이 프롬프트의 철학은 프레게로 부터 시작되었다.'\n",
    ")\n",
    "\n",
    "hub.push('707484c1-574f-4044-921f-46b47251df9e/llm_philosophy', prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca0f78",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

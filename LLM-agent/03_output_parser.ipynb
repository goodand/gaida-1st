{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e5d326",
   "metadata": {},
   "source": [
    "# 출력 파서(output_parser)\n",
    "    '03_output_parser.ipynb'\n",
    "\n",
    "- LLM의 출력을 더 유용/구조화된 형태로 변환함\n",
    "- 구조화 : LLM의 자유 형식 출력을 구조화된 데이터로 변환\n",
    "- 일관성 : 항상 일관된 출력 형식 -> 후속 처리 용이\n",
    "- 유연성 : 다양한 출력 형식(json, list, dict 등)\n",
    "\n",
    "// 내 머릿 속 가설을 검증해보자 자유 형식은 아마도 str, 일관된 출력 형식도 오류 받아서 피드백 반양할듯 쿼리를 다시 요청하거나 변환을 아예 하거나 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad12907",
   "metadata": {},
   "source": [
    "## 'PydanticOutputParser'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d4c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce117473",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_conversation = \"\"\"\n",
    "From: 김철수 (chulsoo.kim@bikecorporation.me)\n",
    "To: 이은채 (eunchae@teddyinternational.me)\n",
    "Subject: \"ZENESIS\" 자전거 유통 협력 및 미팅 일정 제안\n",
    "\n",
    "안녕하세요, 이은채 대리님,\n",
    "\n",
    "저는 바이크코퍼레이션의 김철수 상무입니다. 최근 보도자료를 통해 귀사의 신규 자전거 \"ZENESIS\"에 대해 알게 되었습니다. 바이크코퍼레이션은 자전거 제조 및 유통 분야에서 혁신과 품질을 선도하는 기업으로, 이 분야에서의 장기적인 경험과 전문성을 가지고 있습니다.\n",
    "\n",
    "ZENESIS 모델에 대한 상세한 브로슈어를 요청드립니다. 특히 기술 사양, 배터리 성능, 그리고 디자인 측면에 대한 정보가 필요합니다. 이를 통해 저희가 제안할 유통 전략과 마케팅 계획을 보다 구체화할 수 있을 것입니다.\n",
    "\n",
    "또한, 협력 가능성을 더 깊이 논의하기 위해 다음 주 화요일(1월 15일) 오전 10시에 미팅을 제안합니다. 귀사 사무실에서 만나 이야기를 나눌 수 있을까요?\n",
    "\n",
    "감사합니다.\n",
    "\n",
    "김철수\n",
    "상무이사\n",
    "바이크코퍼레이션\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c5f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래 이메일 내용 중 중요한 것만 추출해 \n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "From: 김철수 (chulsoo.kim@bikecorporation.me)\n",
      "To: 이은채 (eunchae@teddyinternational.me)\n",
      "Subject: \"ZENESIS\" 자전거 유통 협력 및 미팅 일정 제안\n",
      "\n",
      "안녕하세요, 이은채 대리님,\n",
      "\n",
      "저는 바이크코퍼레이션의 김철수 상무입니다. 최근 보도자료를 통해 귀사의 신규 자전거 \"ZENESIS\"에 대해 알게 되었습니다. 바이크코퍼레이션은 자전거 제조 및 유통 분야에서 혁신과 품질을 선도하는 기업으로, 이 분야에서의 장기적인 경험과 전문성을 가지고 있습니다.\n",
      "\n",
      "ZENESIS 모델에 대한 상세한 브로슈어를 요청드립니다. 특히 기술 사양, 배터리 성능, 그리고 디자인 측면에 대한 정보가 필요합니다. 이를 통해 저희가 제안할 유통 전략과 마케팅 계획을 보다 구체화할 수 있을 것입니다.\n",
      "\n",
      "또한, 협력 가능성을 더 깊이 논의하기 위해 다음 주 화요일(1월 15일) 오전 10시에 미팅을 제안합니다. 귀사 사무실에서 만나 이야기를 나눌 수 있을까요?\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "김철수\n",
      "상무이사\n",
      "바이크코퍼레이션\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# prompt = PromptTemplate.from_template(\n",
    "#     \"\"\" 아래 이메일 내용 중 중요한 것만 추출해\n",
    "# ---\n",
    "#     {email_conversation}\"\"\"\n",
    "\n",
    "# )\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    '아래 이메일 내용 중 중요한 것만 추출해 \\n\\n---\\n\\n{email_conversation}'\n",
    ")\n",
    "\n",
    "\n",
    "# print(prompt.format(email_conversation=email_conversation))\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "print('---출력 파서 없는 요약---')\n",
    "chain.invoke({'email_conversation': email_conversation}).content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a492b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"person\": {\"description\": \"매일 보낸 사람\", \"title\": \"Person\", \"type\": \"string\"}, \"email\": {\"description\": \"보낸 사람 메일 주소\", \"title\": \"Email\", \"type\": \"string\"}, \"subject\": {\"description\": \"매일 제목\", \"title\": \"Subject\", \"type\": \"string\"}, \"summary\": {\"description\": \"매일 본문 요약\", \"title\": \"Summary\", \"type\": \"string\"}, \"date\": {\"description\": \"매일 언급된 날짜와 시간\", \"title\": \"Date\", \"type\": \"string\"}}, \"required\": [\"person\", \"email\", \"subject\", \"summary\", \"date\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "class EmailSummary(BaseModel):\n",
    "    person : str = Field(description='매일 보낸 사람')\n",
    "    email : str = Field(description='보낸 사람 메일 주소')\n",
    "    subject : str = Field(description='매일 제목')\n",
    "    summary : str= Field(description='매일 본문 요약')\n",
    "    date : str = Field(description='매일 언급된 날짜와 시간')\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=EmailSummary)\n",
    "\n",
    "print(parser.get_format_instructions()) # 코드를 읽는게 아니라 지침을 LLM이 읽는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb8ee70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question\\n이메일 내용 '], input_types={}, partial_variables={'format': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"person\": {\"description\": \"매일 보낸 사람\", \"title\": \"Person\", \"type\": \"string\"}, \"email\": {\"description\": \"보낸 사람 메일 주소\", \"title\": \"Email\", \"type\": \"string\"}, \"subject\": {\"description\": \"매일 제목\", \"title\": \"Subject\", \"type\": \"string\"}, \"summary\": {\"description\": \"매일 본문 요약\", \"title\": \"Summary\", \"type\": \"string\"}, \"date\": {\"description\": \"매일 언급된 날짜와 시간\", \"title\": \"Date\", \"type\": \"string\"}}, \"required\": [\"person\", \"email\", \"subject\", \"summary\", \"date\"]}\\n```'}, template=' \\n너는 분석철학의 창조자야 아래 질문에 맞게 답변을 한국어로 만들어줘\\n질문 : {question\\n이메일 내용 : {email_conversation}\\n형식 : {format}}\\n    ')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\" \n",
    "너는 분석철학의 창조자야 아래 질문에 맞게 답변을 한국어로 만들어줘\n",
    "질문 : {question\n",
    "이메일 내용 : {email_conversation}\n",
    "형식 : {format}}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 프롬프트 변수들 중 일부만 채우기\n",
    "prompt.partial(format=parser.get_format_instructions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339ffda",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'question\\\\n이메일 내용 '}.  Expected: ['question\\\\n이메일 내용 '] Received: ['question', 'email_conversation']\\nNote: if you intended {question\\n이메일 내용 } to be part of the string and not a variable, please escape it with double curly braces like: '{{question\\n이메일 내용 }}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m chain = prompt | llm | parser\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# res는 객체 \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m res = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m이메일 내용 중 중요한 내용을 추출해줘\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43memail_conversation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43memail_conversation\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m res.model_dump_json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gaida-1st/LLM-agent/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3080\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3079\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3080\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3081\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3082\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gaida-1st/LLM-agent/.venv/lib/python3.13/site-packages/langchain_core/prompts/base.py:216\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    215\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gaida-1st/LLM-agent/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:1953\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1949\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1950\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1951\u001b[39m         output = cast(\n\u001b[32m   1952\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1953\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1955\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1956\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1957\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1958\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1959\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1961\u001b[39m         )\n\u001b[32m   1962\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1963\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gaida-1st/LLM-agent/.venv/lib/python3.13/site-packages/langchain_core/runnables/config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gaida-1st/LLM-agent/.venv/lib/python3.13/site-packages/langchain_core/prompts/base.py:189\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     inner_input_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**inner_input_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gaida-1st/LLM-agent/.venv/lib/python3.13/site-packages/langchain_core/prompts/base.py:183\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    177\u001b[39m     example_key = missing.pop()\n\u001b[32m    178\u001b[39m     msg += (\n\u001b[32m    179\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    184\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[31mKeyError\u001b[39m: \"Input to PromptTemplate is missing variables {'question\\\\n이메일 내용 '}.  Expected: ['question\\\\n이메일 내용 '] Received: ['question', 'email_conversation']\\nNote: if you intended {question\\n이메일 내용 } to be part of the string and not a variable, please escape it with double curly braces like: '{{question\\n이메일 내용 }}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "# res는 객체 \n",
    "res = chain.invoke( \n",
    "    {\n",
    "        \"question\" : '이메일 내용 중 중요한 내용을 추출해줘', \n",
    "        'email_conversation' : email_conversation\n",
    "    }\n",
    ")\n",
    "\n",
    "res.model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ceb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(res.model_dump())\n",
    "\n",
    "# pprint : 이쁘게 뽑기, print의 print "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e94661",
   "metadata": {},
   "source": [
    "# 감정 분석 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b63b9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m알겠습니다. 고객님의 분노와 불편함을 이해하며, 정중하게 환불 요청을 전달하겠습니다. 잠시만 기다려 주세요.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classification(sentiment='중립', aggressiveness=2, language='Korean', remarks='고객의 감정을 이해하며 정중하게 환불 요청을 전달하는 내용입니다.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent,AgentExecutor,create_openai_tools_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pydantic import BaseModel, Field \n",
    "\n",
    "# 1. pydantic 클래스 (답변 형태)\n",
    "class Classification(BaseModel):\n",
    "    sentiment : str = Field(description='글의 감정')\n",
    "    aggressiveness: int = Field(description='얼마나 공격적인지 [1~10]점으로 리커드 스케일로 판단')\n",
    "    language : str = Field(description='작성된 글의 언어')\n",
    "    remarks: str = Field(description='특이사항 요약[30]글자 이내, 동사형으로')\n",
    "\n",
    "# 2. LLM\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano',temperature=0)\n",
    "structured_llm = llm.with_structured_output(Classification) \n",
    "\n",
    "\n",
    "# 3. 프롬프트 정의\n",
    "\n",
    "inp = \"주문한 음식에서 머리카락이 나옴, 분노를 금 할 수 없다. 좋은 말로 할 때 환불을 해라\"\n",
    "# structured_llm.invoke(inp)\n",
    "\n",
    "memory_prompt = f\"\"\" \n",
    "- 확실하지 않으면 'chat_history'를 모두 사용해야 해 \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    ('system', '너는 텍스트에서 감정, 공격성, 언어, 특이사항을 추출하는 분류기야. 지금까지의 대화내용을 종합해서 한국어로 대답해야해'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('human', '{input}'),\n",
    "    MessagesPlaceholder(variable_name='agent_scratchpad')\n",
    "])\n",
    "\n",
    "\n",
    "# 4. 메모리 정의\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages = True,\n",
    "    memory_key = 'chat_history'\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Agent 조립 \n",
    "agent = create_openai_tools_agent(\n",
    "    llm = llm,\n",
    "    tools = [],\n",
    "    prompt = prompt \n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent = agent,\n",
    "    tools = [],\n",
    "    memory = memory,\n",
    "    verbose=True \n",
    ")\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda \n",
    "\n",
    "pipeline = (\n",
    "    agent_executor\n",
    "    | RunnableLambda(lambda d: d['output'])\n",
    "    | structured_llm\n",
    ")\n",
    "\n",
    "pipeline.invoke({'input': inp})\n",
    "# pipline.invoke({'input':inp})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
